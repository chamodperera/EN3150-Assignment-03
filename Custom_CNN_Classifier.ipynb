{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4fe7320",
   "metadata": {},
   "source": [
    "This notebook has been imported from Kaggle. Please note that the dataset used in this notebook is not included here. However, you can find the dataset [here](https://www.kaggle.com/datasets/joebeachcapital/realwaste).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7937a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os, subprocess, sys\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"Physical GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# Optional, see driver details:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for g in gpus:\n",
    "    tf.config.experimental.set_memory_growth(g, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision for T4 Tensor Cores\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de3916",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH = 64\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "DATA_DIR = \"/kaggle/input/realwaste/realwaste-main/RealWaste\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19637c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.30,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "temp_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.30,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "class_names = train_raw.class_names\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff947ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batches = temp_raw.cardinality().numpy()  # should be known for this dataset\n",
    "val_raw  = temp_raw.take(temp_batches // 2)\n",
    "test_raw = temp_raw.skip(temp_batches // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29c696",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02641ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73544469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(ds, num_classes):\n",
    "    counts = np.zeros(num_classes, dtype=np.int64)\n",
    "    for _, y in ds.unbatch():\n",
    "        # y may be int (sparse) or one-hot; normalize to int\n",
    "        if len(y.shape) == 0:\n",
    "            counts[int(y.numpy())] += 1\n",
    "        else:\n",
    "            counts[int(np.argmax(y.numpy()))] += 1\n",
    "    return counts\n",
    "\n",
    "train_counts = count_labels(train_raw, len(class_names))\n",
    "val_counts   = count_labels(val_raw,   len(class_names))\n",
    "test_counts  = count_labels(test_raw,  len(class_names))\n",
    "\n",
    "def bar_counts(title, counts):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.title(title)\n",
    "    plt.bar(class_names, counts)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel(\"images\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "bar_counts(\"Train class distribution\", train_counts)\n",
    "bar_counts(\"Validation class distribution\", val_counts)\n",
    "bar_counts(\"Test class distribution\", test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecb4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "for _, y in train_raw.unbatch().take(2000):\n",
    "    ys.append(y.numpy())\n",
    "\n",
    "ys = np.array(ys)\n",
    "print(\"dtype:\", ys.dtype, \"shape:\", ys.shape)\n",
    "print(\"min/max:\", ys.min(), ys.max())\n",
    "print(\"unique:\", np.unique(ys)[:20]) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
